{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a88882",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfe27d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPs\\\\End to end NLP Project with HuggingFace and Transformers\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45aaa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d97bb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPs\\\\End to end NLP Project with HuggingFace and Transformers'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c3a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065fb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textSummarizer.constants import *\n",
    "from src.textSummarizer.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ded5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path=CONFIG_FILE_PATH,\n",
    "                params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_path)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self)->DataTransformationConfig:\n",
    "        config=self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config=DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a508605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ml_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.textSummarizer.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d370e5a",
   "metadata": {},
   "source": [
    "## Data Transformation Componenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438113a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for train CSV at: D:\\MLOPs\\End to end NLP Project with HuggingFace and Transformers\\artifacts\\data_ingestion\\samsum-train.csv\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the root of your project explicitly\n",
    "project_root = Path(r\"D:\\MLOPs\\End to end NLP Project with HuggingFace and Transformers\")\n",
    "\n",
    "raw_data_dir = project_root / \"artifacts/data_ingestion\"\n",
    "\n",
    "train_csv = raw_data_dir / \"samsum-train.csv\"\n",
    "print(\"Looking for train CSV at:\", train_csv.resolve())\n",
    "print(\"Exists?\", train_csv.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755b87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # ------------------------------\n",
    "# # Configuration class\n",
    "# # ------------------------------\n",
    "# class DataTransformationConfig:\n",
    "#     def __init__(self):\n",
    "#         # Tokenizer and dataset paths\n",
    "#         self.tokenizer_name = \"google/pegasus-cnn_dailymail\"\n",
    "        \n",
    "#         # ✅ Raw CSV dataset folder (directly where CSVs are)\n",
    "#         self.raw_data_dir = Path(\"artifacts/data_ingestion\")\n",
    "        \n",
    "#         # Folder to save tokenized dataset\n",
    "#         self.transformed_data_root = Path(\"artifacts/data_transformation\")\n",
    "#         self.transformed_data_path = self.transformed_data_root / \"samsum_dataset\"\n",
    "\n",
    "#         # Tokenization settings\n",
    "#         self.max_input_length = 512\n",
    "#         self.max_target_length = 128\n",
    "\n",
    "# # ------------------------------\n",
    "# # Data Transformation class\n",
    "# # ------------------------------\n",
    "# class DataTransformation:\n",
    "#     def __init__(self, config: DataTransformationConfig):\n",
    "#         self.config = config\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "#     def convert_examples_to_features(self, example_batch):\n",
    "#         \"\"\"Tokenize dialogue and summary\"\"\"\n",
    "#         inputs = self.tokenizer(\n",
    "#             example_batch['dialogue'],\n",
    "#             max_length=self.config.max_input_length,\n",
    "#             padding='max_length',\n",
    "#             truncation=True\n",
    "#         )\n",
    "#         targets = self.tokenizer(\n",
    "#             example_batch['summary'],\n",
    "#             max_length=self.config.max_target_length,\n",
    "#             padding='max_length',\n",
    "#             truncation=True\n",
    "#         )\n",
    "#         return {\n",
    "#             'input_ids': inputs['input_ids'],\n",
    "#             'attention_mask': inputs['attention_mask'],\n",
    "#             'labels': targets['input_ids']\n",
    "#         }\n",
    "\n",
    "#     def convert(self):\n",
    "#         # 1️⃣ Load CSVs directly from raw_data_dir\n",
    "#         train_csv = self.config.raw_data_dir / \"samsum-train.csv\"\n",
    "#         val_csv = self.config.raw_data_dir / \"samsum-validation.csv\"\n",
    "#         test_csv = self.config.raw_data_dir / \"samsum-test.csv\"\n",
    "\n",
    "#         # ✅ Check if files exist\n",
    "#         for file_path in [train_csv, val_csv, test_csv]:\n",
    "#             if not file_path.exists():\n",
    "#                 raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "\n",
    "#         train_df = pd.read_csv(train_csv)\n",
    "#         val_df = pd.read_csv(val_csv)\n",
    "#         test_df = pd.read_csv(test_csv)\n",
    "\n",
    "#         # 2️⃣ Convert pandas DataFrames to Hugging Face Datasets\n",
    "#         dataset = DatasetDict({\n",
    "#             'train': Dataset.from_pandas(train_df),\n",
    "#             'validation': Dataset.from_pandas(val_df),\n",
    "#             'test': Dataset.from_pandas(test_df)\n",
    "#         })\n",
    "\n",
    "#         # 3️⃣ Tokenize datasets\n",
    "#         tokenized_dataset = dataset.map(self.convert_examples_to_features, batched=True)\n",
    "\n",
    "#         # 4️⃣ Save tokenized dataset\n",
    "#         os.makedirs(self.config.transformed_data_root, exist_ok=True)\n",
    "#         tokenized_dataset.save_to_disk(self.config.transformed_data_path)\n",
    "#         print(f\"Tokenized dataset saved at {self.config.transformed_data_path}\")\n",
    "\n",
    "# # ------------------------------\n",
    "# # Usage\n",
    "# # ------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     config = DataTransformationConfig()\n",
    "#     transformer = DataTransformation(config=config)\n",
    "#     transformer.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd65234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of train data after cleaning:\n",
      "         id                                           dialogue  \\\n",
      "0  13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
      "1  13728867  Olivia: Who are you voting for in this electio...   \n",
      "2  13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
      "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4  13728094  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
      "\n",
      "                                             summary  \n",
      "0  Amanda baked cookies and will bring Jerry some...  \n",
      "1  Olivia and Olivier are voting for liberals in ...  \n",
      "2  Kim may try the pomodoro technique recommended...  \n",
      "3  Edward thinks he is in love with Bella. Rachel...  \n",
      "4  Sam is confused, because he overheard Rick com...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14731/14731 [00:09<00:00, 1502.42 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 1959.24 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 1395.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 14731/14731 [00:00<00:00, 190682.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 818/818 [00:00<00:00, 48196.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 819/819 [00:00<00:00, 53044.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenized dataset saved at D:\\MLOPs\\End to end NLP Project with HuggingFace and Transformers\\artifacts\\data_transformation\\samsum_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration class\n",
    "# ------------------------------\n",
    "class DataTransformationConfig:\n",
    "    def __init__(self):\n",
    "        # Tokenizer and dataset paths\n",
    "        self.tokenizer_name = \"google/pegasus-cnn_dailymail\"\n",
    "        \n",
    "        # ✅ Raw CSV dataset folder (directly where CSVs are)\n",
    "        self.raw_data_dir = Path(\"artifacts/data_ingestion\")\n",
    "        \n",
    "        # Folder to save tokenized dataset\n",
    "        self.transformed_data_root = Path(\"artifacts/data_transformation\")\n",
    "        self.transformed_data_path = self.transformed_data_root / \"samsum_dataset\"\n",
    "\n",
    "        # Tokenization settings\n",
    "        self.max_input_length = 512\n",
    "        self.max_target_length = 128\n",
    "\n",
    "# ------------------------------\n",
    "# Data Transformation class\n",
    "# ------------------------------\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "    def convert_examples_to_features(self, example_batch):\n",
    "        \"\"\"Tokenize dialogue and summary safely\"\"\"\n",
    "        # ✅ Ensure all values are strings and handle NaN properly\n",
    "        dialogues = [str(x) if x is not None else \"\" for x in example_batch['dialogue']]\n",
    "        summaries = [str(x) if x is not None else \"\" for x in example_batch['summary']]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            dialogues,\n",
    "            max_length=self.config.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            summaries,\n",
    "            max_length=self.config.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'labels': targets['input_ids']\n",
    "        }\n",
    "\n",
    "    def convert(self):\n",
    "        # 1️⃣ Load CSVs directly from raw_data_dir\n",
    "        train_csv = self.config.raw_data_dir / \"samsum-train.csv\"\n",
    "        val_csv = self.config.raw_data_dir / \"samsum-validation.csv\"\n",
    "        test_csv = self.config.raw_data_dir / \"samsum-test.csv\"\n",
    "\n",
    "        # ✅ Check if files exist\n",
    "        for file_path in [train_csv, val_csv, test_csv]:\n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "\n",
    "        # ✅ Read CSVs and drop rows with missing dialogue/summary\n",
    "        train_df = pd.read_csv(train_csv).dropna(subset=['dialogue', 'summary'])\n",
    "        val_df = pd.read_csv(val_csv).dropna(subset=['dialogue', 'summary'])\n",
    "        test_df = pd.read_csv(test_csv).dropna(subset=['dialogue', 'summary'])\n",
    "\n",
    "        # Debug: Print small sample before tokenization\n",
    "        print(\"Sample of train data after cleaning:\")\n",
    "        print(train_df.head())\n",
    "\n",
    "        # 2️⃣ Convert pandas DataFrames to Hugging Face Datasets\n",
    "        dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_df),\n",
    "            'validation': Dataset.from_pandas(val_df),\n",
    "            'test': Dataset.from_pandas(test_df)\n",
    "        })\n",
    "\n",
    "        # 3️⃣ Tokenize datasets\n",
    "        tokenized_dataset = dataset.map(self.convert_examples_to_features, batched=True)\n",
    "\n",
    "        # 4️⃣ Save tokenized dataset\n",
    "        os.makedirs(self.config.transformed_data_root, exist_ok=True)\n",
    "        tokenized_dataset.save_to_disk(self.config.transformed_data_path)\n",
    "        print(f\"✅ Tokenized dataset saved at {self.config.transformed_data_path.resolve()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Usage\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = DataTransformationConfig()\n",
    "    transformer = DataTransformation(config=config)\n",
    "    transformer.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33116f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14731\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n",
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.', '__index_level_0__': 0, 'input_ids': [12195, 151, 125, 7091, 3659, 107, 842, 119, 245, 181, 152, 10508, 151, 7435, 147, 12195, 151, 125, 131, 267, 650, 119, 3469, 29344, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = \"D:/MLOPs/End to end NLP Project with HuggingFace and Transformers/artifacts/data_transformation/samsum_dataset\"\n",
    "dataset_samsum_pt = load_from_disk(dataset_path)\n",
    "\n",
    "print(dataset_samsum_pt)\n",
    "print(dataset_samsum_pt[\"train\"][0])  # just to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eca014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
